{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2be08d33-dd1d-4071-99f0-9997b922d0ce",
   "metadata": {},
   "source": [
    "# Convolutional SNN\n",
    "### **Classifying Fashion-MNIST with Convolutional SNN**\n",
    "\n",
    "This tutorial goes over how to train a convolutional spiking neural network (CSNN) on the Fashion-MNIST dataset and deploy on HiAER Spike using our conversion pipline.\n",
    "\n",
    "### **Define a CSNN**\n",
    "To build a CSNN with PyTorch, we can use snnTorch, SpikingJelly or other deep learning frameworks that are based on PyTorch. Currently, our conversion pipline supports snnTorch and SpikingJelly. In this tutorial, we will be using SpikingJelly.\n",
    "\n",
    "Install the PyPi distribution of SpikingJelly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83016a2-f759-47a1-a229-4266d4830efd",
   "metadata": {},
   "source": [
    "Import necessary libraries from SpikingJelly and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187bc8fa-aff2-41c8-b11c-9e062fe183b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikingjelly.activation_based import neuron, functional, surrogate, layer\n",
    "import torch \n",
    "import os\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import tifffile as tiff\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1786bffa-4f0f-4525-9879-a65dbc2e6f29",
   "metadata": {},
   "source": [
    "### **Model Architecture**\n",
    "Using SpikingJelly, we can define a CSNN with the architecture of 8C3-BN-6272FC10\n",
    "- 8C3: a 3x3 convolutional kernel with 8 channels\n",
    "- BN: batch normalization layer \n",
    "- 6272FC10: the fully connected output layer \n",
    " \n",
    "#### **Surrogate Function**\n",
    "SpikingJelly and snnTorch both use backpropagation through time to train the spiking neural networks. However, because of the non-differentiability of spikes, surrogate gradients are used in place of the Heaviside function in the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d0ab95c-1fc5-4a9c-b007-aa03197bfd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module): \n",
    "    def __init__(self, channels=8): \n",
    "        super().__init__()\n",
    "        #first block\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 2, kernel_size=1, padding=1, bias=False)\n",
    "        self.lif1 = neuron.IFNode(surrogate_function=surrogate.ATan())\n",
    "        self.conv2 = nn.Conv2d(in_channels = 2, out_channels = 12, kernel_size=3, padding=1, bias=False)\n",
    "        self.lif2 = neuron.IFNode(surrogate_function=surrogate.ATan())\n",
    "        self.conv3 = nn.Conv2d(in_channels = 12, out_channels = 12, kernel_size=3, padding=1, bias=False)\n",
    "        self.lif3 = neuron.IFNode(surrogate_function=surrogate.ATan())\n",
    "        \n",
    "        #downsample\n",
    "        self.conv4 = nn.Conv2d(in_channels = 12,out_channels = 24, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        #upsample\n",
    "        self.conv5 = nn.ConvTranspose2d(in_channels = 24,out_channels = 12, kernel_size=2, stride=2, bias=False)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(in_channels = 24,out_channels = 12, kernel_size = 3, padding=1, bias=False)\n",
    "        self.lif4 = neuron.IFNode(surrogate_function=surrogate.ATan())\n",
    "        self.conv7 = nn.Conv2d(in_channels = 12,out_channels = 12, kernel_size = 3, padding=1, bias=False)\n",
    "        self.lif5 = neuron.IFNode(surrogate_function=surrogate.ATan())\n",
    "        self.conv8 = nn.Conv2d(in_channels=12, out_channels= 2, kernel_size=1, bias=False)\n",
    "        self.lif6 = neuron.IFNode(surrogate_function=surrogate.ATan())\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.lif1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.lif2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.lif3(x)\n",
    "        #crop\n",
    "        y = v2.CenterCrop(size=54)(x)\n",
    "        #downsample\n",
    "        x = self.conv4(x)\n",
    "        x = v2.CenterCrop(size=27 )(x)\n",
    "        z = self.conv5(x)\n",
    "        #print(y.size())\n",
    "        #print(z.size())\n",
    "        x = torch.concatenate([y,z], axis=1)\n",
    "        #print(x.size())\n",
    "        x = self.conv6(x)\n",
    "        x = self.lif4(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.lif5(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.lif6(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d0c7a1b-4357-4a41-85ca-9ca56bdb86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate the Network\n",
    "net = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af49d6d4-5124-4375-aa5c-6b98ded83a6f",
   "metadata": {},
   "source": [
    "### **Setting up the MNIST Dataset**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "968b2f55-4a87-48eb-bdae-8efdbd6b2f16",
   "metadata": {},
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Download Fashion-MNIST data from torch \n",
    "fashion_mnist_train = datasets.FashionMNIST('data/fashion_mnist', train=True, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]))\n",
    "fashion_mnist_test = datasets.FashionMNIST('data/fashion_mnist', train=False, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(fashion_mnist_train, batch_size=128, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(fashion_mnist_test, batch_size=128, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76b0be19-2449-4f58-8c1f-deb086a17ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EMDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, test=False):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform        \n",
    "        if test:\n",
    "            self.img_trn = tiff.imread(os.path.join(root_dir,'test-volume.tif'))\n",
    "            self.msk_trn = tiff.imread(os.path.join(root_dir,'test-labels.tif'))\n",
    "        else:\n",
    "            self.img_trn = tiff.imread(os.path.join(root_dir,'train-volume.tif'))\n",
    "            self.msk_trn = tiff.imread(os.path.join(root_dir,'train-labels.tif'))\n",
    "        expand = np.zeros((len(self.msk_trn),2,512,512))\n",
    "        for idx,mask in enumerate(self.msk_trn):\n",
    "            expand[idx,0,:,:] = mask==0\n",
    "            expand[idx,1,:,:] = mask==255\n",
    "        self.msk_trn = expand\n",
    "        self.msk_trn.astype(float)\n",
    "        print(np.shape(self.msk_trn))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_trn)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = self.img_trn[idx]\n",
    "        mask = self.msk_trn[idx]\n",
    "        #extract the channels\n",
    "        \n",
    "        \n",
    "        sample = {'image': image, 'mask': mask}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b85cd4fc-fb84-4c6e-a02d-589ce7b7e4b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "        msk = transform.resize(mask,  (2, new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        #landmarks = landmarks * [new_w / w, new_h / h]\n",
    "\n",
    "        return {'image': img, 'mask': mask}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = torch.randint(low = 0, high = h - new_h + 1, size=(1,1))\n",
    "        left = torch.randint(low = 0, high= w - new_w + 1, size=(1,1))\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        mask = mask[:,top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        return {'image': image, 'mask': mask}\n",
    "    \n",
    "class flipHorizontal(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        \n",
    "        # Random horizontal flipping\n",
    "        rand_int = torch.randint(low = 0, high = 1, size=(1,1))\n",
    "        if rand_int > 0.5:\n",
    "            image = F.hflip(image)\n",
    "            mask = F.hflip(mask)\n",
    "\n",
    "        return {'image': image, 'mask': mask}\n",
    "    \n",
    "class flipVertical(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        \n",
    "        # Random horizontal flipping\n",
    "        rand_int = torch.randint(low = 0, high = 1, size=(1,1))\n",
    "        if rand_int > 0.5:\n",
    "            image = F.vflip(image)\n",
    "            mask = F.vflip(mask)\n",
    "\n",
    "        return {'image': image, 'mask': mask}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        image = image[None,:,:]\n",
    "        #image = image[None,:,:]\n",
    "        #image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'mask': torch.from_numpy(mask)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c68aa0e3-ef2e-4714-8cfa-41bdbe68fd13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 2, 512, 512)\n",
      "(30, 2, 512, 512)\n",
      "0 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "1 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "2 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "3 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "4 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "5 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "6 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "7 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "8 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "9 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "10 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "11 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "12 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "13 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "14 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "15 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "16 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "17 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "18 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "19 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "20 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "21 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "22 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "23 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "24 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "25 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "26 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "27 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "28 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n",
      "29 torch.Size([1, 64, 64]) torch.Size([2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "em_dataset = EMDataset(root_dir='/Users/gweneverefrank/code/hs_api/examples/ISBI-2012-challenge',  transform=transforms.Compose([\n",
    "                                               Rescale(192),\n",
    "                                               RandomCrop(64),\n",
    "                                               ToTensor(),\n",
    "                                               flipHorizontal(),\n",
    "                                               flipVertical()\n",
    "                                           ]))\n",
    "\n",
    "em_dataset_test = EMDataset(root_dir='/Users/gweneverefrank/code/hs_api/examples/ISBI-2012-challenge',  transform=transforms.Compose([\n",
    "                                               Rescale(192),\n",
    "                                               RandomCrop(64),\n",
    "                                               ToTensor()\n",
    "                                           ]),test=True)\n",
    "\n",
    "\n",
    "for i, sample in enumerate(em_dataset):\n",
    "    print(i, sample['image'].shape, sample['mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dc17714-516e-4ad5-9ac1-b5726f12d2db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "1 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "2 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "3 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "4 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "5 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "6 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "7 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "8 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "9 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "10 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "11 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "12 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "13 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "14 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "15 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "16 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "17 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "18 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "19 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "20 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "21 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "22 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "23 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "24 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "25 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "26 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "27 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "28 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n",
      "29 torch.Size([1, 1, 64, 64]) torch.Size([1, 2, 64, 64])\n",
      "type: torch.DoubleTensor torch.DoubleTensor\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(em_dataset, batch_size=1,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "test_loader = DataLoader(em_dataset_test, batch_size=1,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "for i, sample in enumerate(train_loader):\n",
    "    print(i, sample['image'].shape, sample['mask'].shape)\n",
    "    print('type: '+str(sample['image'].type())+' '+str(sample['mask'].type()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba1b524-014e-4a13-81e3-4d918ed28c12",
   "metadata": {},
   "source": [
    "### **Training the SNN**\n",
    "Since we are using a static image dataset, we will first encode the image into spikes using the rate encoding function from spikingjelly. With rate encoding, the input feature determines the firing frequency and the neuron that fries the most is selected as the predicted class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a83e51ba-4964-49c7-8b92-969923d8febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikingjelly.activation_based import encoding\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "836f14da-e64b-41be-b20f-852a2d697de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the encoder and the time steps\n",
    "encoder = encoding.PoissonEncoder()\n",
    "num_steps = 40\n",
    "\n",
    "#Define training parameters\n",
    "epochs = 40\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "#Copy netowrk to device \n",
    "net.to(device)\n",
    "\n",
    "#Define optimizer, scheduler and the loss function\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "loss_fun = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c61ca37-a10e-4412-bc60-e5b13da35f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78d597ac-f886-454e-b245-593e72e0e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished a bout\n",
      "epoch = 0, train_loss = 0.6876, train_acc = 0.4770, test_loss = 0.6661, test_acc = 0.7501\n",
      "train speed = 5.9364 images/s, test speed = 12.9118 images/s\n",
      "finished a bout\n",
      "epoch = 1, train_loss = 0.6461, train_acc = 0.7494, test_loss = 0.6376, test_acc = 0.7345\n",
      "train speed = 5.9349 images/s, test speed = 12.8414 images/s\n",
      "finished a bout\n",
      "epoch = 2, train_loss = 0.6119, train_acc = 0.7980, test_loss = 0.6202, test_acc = 0.7762\n",
      "train speed = 5.9337 images/s, test speed = 12.8454 images/s\n",
      "finished a bout\n",
      "epoch = 3, train_loss = 0.6286, train_acc = 0.7536, test_loss = 0.6318, test_acc = 0.7449\n",
      "train speed = 5.9708 images/s, test speed = 12.6790 images/s\n",
      "finished a bout\n",
      "epoch = 4, train_loss = 0.6148, train_acc = 0.7818, test_loss = 0.6132, test_acc = 0.7850\n",
      "train speed = 5.8946 images/s, test speed = 12.8548 images/s\n",
      "finished a bout\n",
      "epoch = 5, train_loss = 0.6240, train_acc = 0.7619, test_loss = 0.6354, test_acc = 0.7370\n",
      "train speed = 5.8511 images/s, test speed = 12.9549 images/s\n",
      "finished a bout\n",
      "epoch = 6, train_loss = 0.6142, train_acc = 0.7828, test_loss = 0.6235, test_acc = 0.7623\n",
      "train speed = 5.7943 images/s, test speed = 12.7536 images/s\n",
      "finished a bout\n",
      "epoch = 7, train_loss = 0.6117, train_acc = 0.7876, test_loss = 0.6253, test_acc = 0.7579\n",
      "train speed = 5.8216 images/s, test speed = 12.9909 images/s\n",
      "finished a bout\n",
      "epoch = 8, train_loss = 0.6302, train_acc = 0.7476, test_loss = 0.6294, test_acc = 0.7491\n",
      "train speed = 5.7221 images/s, test speed = 12.6288 images/s\n",
      "finished a bout\n",
      "epoch = 9, train_loss = 0.6221, train_acc = 0.7648, test_loss = 0.6167, test_acc = 0.7759\n",
      "train speed = 5.6341 images/s, test speed = 12.9802 images/s\n",
      "finished a bout\n",
      "epoch = 10, train_loss = 0.6190, train_acc = 0.7710, test_loss = 0.6388, test_acc = 0.7294\n",
      "train speed = 5.7770 images/s, test speed = 12.9736 images/s\n",
      "finished a bout\n",
      "epoch = 11, train_loss = 0.6194, train_acc = 0.7705, test_loss = 0.6164, test_acc = 0.7764\n",
      "train speed = 5.7096 images/s, test speed = 12.9208 images/s\n",
      "finished a bout\n",
      "epoch = 12, train_loss = 0.6241, train_acc = 0.7598, test_loss = 0.6211, test_acc = 0.7663\n",
      "train speed = 5.7840 images/s, test speed = 12.8497 images/s\n",
      "finished a bout\n",
      "epoch = 13, train_loss = 0.6202, train_acc = 0.7682, test_loss = 0.6260, test_acc = 0.7559\n",
      "train speed = 5.7617 images/s, test speed = 12.5636 images/s\n",
      "finished a bout\n",
      "epoch = 14, train_loss = 0.6245, train_acc = 0.7590, test_loss = 0.6086, test_acc = 0.7920\n",
      "train speed = 5.7158 images/s, test speed = 12.5421 images/s\n",
      "finished a bout\n",
      "epoch = 15, train_loss = 0.6246, train_acc = 0.7590, test_loss = 0.6279, test_acc = 0.7518\n",
      "train speed = 5.7741 images/s, test speed = 12.6082 images/s\n",
      "finished a bout\n",
      "epoch = 16, train_loss = 0.6185, train_acc = 0.7715, test_loss = 0.6183, test_acc = 0.7717\n",
      "train speed = 5.7670 images/s, test speed = 12.6672 images/s\n",
      "finished a bout\n",
      "epoch = 17, train_loss = 0.6155, train_acc = 0.7776, test_loss = 0.5981, test_acc = 0.8140\n",
      "train speed = 5.7961 images/s, test speed = 12.7654 images/s\n",
      "finished a bout\n",
      "epoch = 18, train_loss = 0.6180, train_acc = 0.7721, test_loss = 0.6292, test_acc = 0.7488\n",
      "train speed = 5.7627 images/s, test speed = 12.7916 images/s\n",
      "finished a bout\n",
      "epoch = 19, train_loss = 0.6194, train_acc = 0.7692, test_loss = 0.6327, test_acc = 0.7415\n",
      "train speed = 5.8014 images/s, test speed = 12.8378 images/s\n",
      "finished a bout\n",
      "epoch = 20, train_loss = 0.6223, train_acc = 0.7635, test_loss = 0.6345, test_acc = 0.7372\n",
      "train speed = 5.7635 images/s, test speed = 12.6805 images/s\n",
      "finished a bout\n",
      "epoch = 21, train_loss = 0.6331, train_acc = 0.7408, test_loss = 0.6149, test_acc = 0.7788\n",
      "train speed = 5.7498 images/s, test speed = 12.7296 images/s\n",
      "finished a bout\n",
      "epoch = 22, train_loss = 0.6175, train_acc = 0.7732, test_loss = 0.6128, test_acc = 0.7829\n",
      "train speed = 5.7601 images/s, test speed = 12.6962 images/s\n",
      "finished a bout\n",
      "epoch = 23, train_loss = 0.6333, train_acc = 0.7404, test_loss = 0.6166, test_acc = 0.7748\n",
      "train speed = 5.7006 images/s, test speed = 12.8017 images/s\n",
      "finished a bout\n",
      "epoch = 24, train_loss = 0.6229, train_acc = 0.7620, test_loss = 0.6154, test_acc = 0.7778\n",
      "train speed = 5.7834 images/s, test speed = 12.7805 images/s\n",
      "finished a bout\n",
      "epoch = 25, train_loss = 0.6155, train_acc = 0.7774, test_loss = 0.6473, test_acc = 0.7112\n",
      "train speed = 5.7412 images/s, test speed = 12.7809 images/s\n",
      "finished a bout\n",
      "epoch = 26, train_loss = 0.6236, train_acc = 0.7605, test_loss = 0.6167, test_acc = 0.7744\n",
      "train speed = 5.7352 images/s, test speed = 12.1951 images/s\n",
      "finished a bout\n",
      "epoch = 27, train_loss = 0.6150, train_acc = 0.7782, test_loss = 0.6253, test_acc = 0.7569\n",
      "train speed = 5.6294 images/s, test speed = 11.8164 images/s\n",
      "finished a bout\n",
      "epoch = 28, train_loss = 0.6067, train_acc = 0.7955, test_loss = 0.6370, test_acc = 0.7324\n",
      "train speed = 5.7159 images/s, test speed = 12.4196 images/s\n",
      "finished a bout\n",
      "epoch = 29, train_loss = 0.6078, train_acc = 0.7928, test_loss = 0.6176, test_acc = 0.7726\n",
      "train speed = 5.7218 images/s, test speed = 12.4618 images/s\n",
      "finished a bout\n",
      "epoch = 30, train_loss = 0.6082, train_acc = 0.7921, test_loss = 0.6344, test_acc = 0.7381\n",
      "train speed = 5.7749 images/s, test speed = 12.4699 images/s\n",
      "finished a bout\n",
      "epoch = 31, train_loss = 0.6326, train_acc = 0.7418, test_loss = 0.6271, test_acc = 0.7531\n",
      "train speed = 5.7097 images/s, test speed = 12.7139 images/s\n",
      "finished a bout\n",
      "epoch = 32, train_loss = 0.6089, train_acc = 0.7907, test_loss = 0.6060, test_acc = 0.7969\n",
      "train speed = 5.7521 images/s, test speed = 12.8183 images/s\n",
      "finished a bout\n",
      "epoch = 33, train_loss = 0.6165, train_acc = 0.7751, test_loss = 0.6202, test_acc = 0.7671\n",
      "train speed = 5.7445 images/s, test speed = 12.7162 images/s\n",
      "finished a bout\n",
      "epoch = 34, train_loss = 0.6348, train_acc = 0.7371, test_loss = 0.6206, test_acc = 0.7661\n",
      "train speed = 5.7591 images/s, test speed = 12.4320 images/s\n",
      "finished a bout\n",
      "epoch = 35, train_loss = 0.6252, train_acc = 0.7570, test_loss = 0.6085, test_acc = 0.7915\n",
      "train speed = 5.6578 images/s, test speed = 12.7053 images/s\n",
      "finished a bout\n",
      "epoch = 36, train_loss = 0.6154, train_acc = 0.7772, test_loss = 0.6425, test_acc = 0.7216\n",
      "train speed = 5.7715 images/s, test speed = 12.5578 images/s\n",
      "finished a bout\n",
      "epoch = 37, train_loss = 0.6107, train_acc = 0.7870, test_loss = 0.6062, test_acc = 0.7961\n",
      "train speed = 5.7636 images/s, test speed = 12.6073 images/s\n",
      "finished a bout\n",
      "epoch = 38, train_loss = 0.6214, train_acc = 0.7648, test_loss = 0.6079, test_acc = 0.7929\n",
      "train speed = 5.7715 images/s, test speed = 12.5502 images/s\n",
      "finished a bout\n",
      "epoch = 39, train_loss = 0.6162, train_acc = 0.7755, test_loss = 0.6255, test_acc = 0.7561\n",
      "train speed = 5.7424 images/s, test speed = 12.6048 images/s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_samples = 0\n",
    "    for sample in train_loader:\n",
    "        img = sample['image']\n",
    "        mask = sample['mask']\n",
    "        optimizer.zero_grad()\n",
    "        img = img.to(device)\n",
    "        mask = mask.to(device)\n",
    "        #label_onehot = torch.nn.functional.one_hot(label, 10).float()\n",
    "        out_fr = 0.\n",
    "        for t in range(num_steps):\n",
    "            #print(img.size())\n",
    "            encoded_img = encoder(img)\n",
    "            #print(encoded_img.size())\n",
    "            out_fr += net(encoded_img.float())\n",
    "        out_fr = out_fr/num_steps\n",
    "        #print('outputsize')\n",
    "        #print(out_fr.size())\n",
    "        #out_fr = out_fr[None, :, :, :]\n",
    "        out_fr = F.interpolate(out_fr, (64,64))\n",
    "        #out_fr = torch.softmax(out_fr,dim=1).float()\n",
    "        loss = loss_fun(out_fr.float(), mask.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_samples += 1\n",
    "        train_loss += loss.item()\n",
    "        class_labels = mask.argmax(1)\n",
    "        output_labels = out_fr.argmax(1)\n",
    "        train_acc += (output_labels == class_labels).float().sum().item() / (64**2)\n",
    "\n",
    "        #reset the membrane protential after each input image\n",
    "        functional.reset_net(net)\n",
    "\n",
    "    train_time = time.time()\n",
    "    train_speed = train_samples / (train_time - start_time)\n",
    "    train_loss /= train_samples\n",
    "    train_acc /= train_samples\n",
    "    print('finished a bout')\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "        \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    test_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample in test_loader:\n",
    "            img = sample['image']\n",
    "            mask = sample['mask']\n",
    "            img = img.to(device)\n",
    "            mask = mask.to(device)\n",
    "            out_fr = 0.   \n",
    "            for t in range(num_steps):\n",
    "                #print(img.size())\n",
    "                encoded_img = encoder(img)\n",
    "                #print(encoded_img.size())\n",
    "                out_fr += net(encoded_img.float())\n",
    "            out_fr = out_fr/num_steps\n",
    "            #print(out_fr.size())\n",
    "            #out_fr = out_fr[None, :, :, :]\n",
    "            out_fr = F.interpolate(out_fr, (64,64))\n",
    "            loss = loss_fun(out_fr, mask)\n",
    "\n",
    "            \n",
    "            \n",
    "            test_samples += 1\n",
    "            test_loss += loss.item()\n",
    "            test_class_labels = mask.argmax(1)\n",
    "            test_output_labels = out_fr.argmax(1)\n",
    "            test_acc += (test_output_labels == test_class_labels).float().sum().item() / (64**2)\n",
    "            functional.reset_net(net)\n",
    "\n",
    "\n",
    "    test_time = time.time()\n",
    "    test_speed = test_samples / (test_time - train_time)\n",
    "    test_loss /= test_samples\n",
    "    test_acc /= test_samples\n",
    "\n",
    "    print(f'epoch = {epoch}, train_loss ={train_loss: .4f}, train_acc ={train_acc: .4f}, test_loss ={test_loss: .4f}, test_acc ={test_acc: .4f}')\n",
    "    print(f'train speed ={train_speed: .4f} images/s, test speed ={test_speed: .4f} images/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd56c3d-ecf4-47e6-97e6-e6dd9986975e",
   "metadata": {},
   "source": [
    "### **Converting the trained SNN to HiAER Spike Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4153ff3-0b42-4101-b438-ff8602495150",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Quantize_Network.__init__() missing 1 required positional argument: 'w_alpha'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m net_bn \u001b[38;5;241m=\u001b[39m bn\u001b[38;5;241m.\u001b[39mfold(net)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#Weight, Bias Quantization \u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m qn \u001b[38;5;241m=\u001b[39m \u001b[43mQuantize_Network\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     11\u001b[0m net_quan \u001b[38;5;241m=\u001b[39m qn\u001b[38;5;241m.\u001b[39mquantize(net_bn)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#Set the parameters for conversion\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Quantize_Network.__init__() missing 1 required positional argument: 'w_alpha'"
     ]
    }
   ],
   "source": [
    "from hs_api.converter import CRI_Converter, Quantize_Network, BN_Folder\n",
    "from hs_api.api import CRI_network\n",
    "# import hs_bridge #Uncomment when running on FPGA\n",
    "\n",
    "#Fold the BN layer \n",
    "bn = BN_Folder() \n",
    "net_bn = bn.fold(net)\n",
    "\n",
    "#Weight, Bias Quantization \n",
    "qn = Quantize_Network() \n",
    "net_quan = qn.quantize(net_bn)\n",
    "\n",
    "#Set the parameters for conversion\n",
    "input_layer = 0 #first pytorch layer that acts as synapses\n",
    "output_layer = 4 #last pytorch layer that acts as synapses\n",
    "input_shape = (1, 28, 28)\n",
    "backend = 'spikingjelly'\n",
    "v_threshold = qn.v_threshold\n",
    "\n",
    "cn = CRI_Converter(num_steps = num_steps, \n",
    "                   input_layer = input_layer, \n",
    "                   output_layer = output_layer, \n",
    "                   input_shape = input_shape,\n",
    "                   backend=backend,\n",
    "                   v_threshold = v_threshold)\n",
    "cn.layer_converter(net_quan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d65ba4-c50c-4424-90fe-b1058e27c049",
   "metadata": {},
   "source": [
    "### **Initiate the HiAER Spike SNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5575e9-5974-4c0e-abf4-06bd5fb3315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['neuron_type'] = \"I&F\"\n",
    "config['global_neuron_params'] = {}\n",
    "config['global_neuron_params']['v_thr'] = int(quan_fun.v_threshold)\n",
    "    \n",
    "# #Uncomment this to create a network running on the FPGA\n",
    "# hardwareNetwork = CRI_network(dict(cri_convert.axon_dict),\n",
    "#                               connections=dict(cri_convert.neuron_dict),\n",
    "#                               config=config,target='CRI', \n",
    "#                               outputs = cri_convert.output_neurons,\n",
    "#                               coreID=1)\n",
    "\n",
    "softwareNetwork = CRI_network(dict(cri_convert.axon_dict),\n",
    "                              connections=dict(cri_convert.neuron_dict),\n",
    "                              config=config,target='simpleSim', \n",
    "                              outputs = cri_convert.output_neurons,\n",
    "                              coreID=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a3cfbd-df94-4546-8ca3-8689b8a25719",
   "metadata": {},
   "source": [
    "### **Deploying the SNN on HiAER Spike**\n",
    "\n",
    "run_sw and run_hw are two helper functions for running the spiking neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc80f35-b960-49c2-9631-b7981cf23d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_sw(self,inputList,softwareNetwork):\n",
    "    predictions = []\n",
    "    total_time_cri = 0\n",
    "    #each image\n",
    "    for currInput in tqdm(inputList):\n",
    "        #reset the membrane potential to zero\n",
    "        softwareNetwork.simpleSim.initialize_sim_vars(len(self.neuron_dict))\n",
    "        spikeRate = [0]*10\n",
    "        #each time step\n",
    "        for slice in currInput:\n",
    "            start_time = time.time()\n",
    "            swSpike = softwareNetwork.step(slice, membranePotential=False)\n",
    "\n",
    "            end_time = time.time()\n",
    "            total_time_cri = total_time_cri + end_time-start_time\n",
    "            for spike in swSpike:\n",
    "                spikeIdx = int(spike) - self.bias_start_idx \n",
    "                try: \n",
    "                    if spikeIdx >= 0: \n",
    "                        spikeRate[spikeIdx] += 1 \n",
    "                except:\n",
    "                    print(\"SpikeIdx: \", spikeIdx,\"\\n SpikeRate:\",spikeRate)\n",
    "        predictions.append(spikeRate.index(max(spikeRate)))\n",
    "    print(f\"Total simulation execution time: {total_time_cri:.5f} s\")\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669d786-cbab-4b5c-9b4c-ab2db0baeb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CRI_hw(self,inputList,hardwareNetwork):\n",
    "    predictions = []\n",
    "    #each image\n",
    "    total_time_cri = 0\n",
    "    for currInput in tqdm(inputList):\n",
    "        #initiate the hardware for each image\n",
    "        hs_bridge.FPGA_Execution.fpga_controller.clear(len(self.neuron_dict), False, 0)  ##Num_neurons, simDump, coreOverride\n",
    "        spikeRate = [0]*10\n",
    "        #each time step\n",
    "        for slice in tqdm(currInput):\n",
    "            start_time = time.time()\n",
    "            hwSpike, latency, hbmAcc = hardwareNetwork.step(slice, membranePotential=False)\n",
    "            print(f'hwSpike: {hwSpike}\\n. latency : {latency}\\n. hbmAcc:{hbmAcc}')\n",
    "            end_time = time.time()\n",
    "            total_time_cri = total_time_cri + end_time-start_time\n",
    "            for spike in hwSpike:\n",
    "                # print(int(spike))\n",
    "                spikeIdx = int(spike) - self.bias_start_idx \n",
    "                try: \n",
    "                    if spikeIdx >= 0: \n",
    "                        spikeRate[spikeIdx] += 1 \n",
    "                except:\n",
    "                    print(\"SpikeIdx: \", spikeIdx,\"\\n SpikeRate:\",spikeRate)\n",
    "        predictions.append(spikeRate.index(max(spikeRate))) \n",
    "    print(f\"Total execution time CRIFPGA: {total_time_cri:.5f} s\")\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ec6501-bb74-4f49-a7d0-dc8efc4baad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cri_convert.bias_start_idx = int(cri_convert.output_neurons[0])\n",
    "loss_fun = nn.MSELoss()\n",
    "start_time = time.time()\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "test_samples = 0\n",
    "num_batches = 0\n",
    "\n",
    "RUN_HARDWARE = False #Set to True if running on FPGA\n",
    "\n",
    "for img, label in tqdm(test_loader):\n",
    "    cri_input = cri_convert.input_converter(img)\n",
    "    output = None\n",
    "    if RUN_HARDWARE:\n",
    "        output = torch.tensor(run_CRI_hw(cri_input,hardwareNetwork), dtype=float)\n",
    "    else:\n",
    "        output = torch.tensor(run_CRI_sw(cri_input,softwareNetwork), dtype=float)\n",
    "    loss = loss_fun(output, label)\n",
    "    test_samples += label.numel()\n",
    "    test_loss += loss.item() * label.numel()\n",
    "    test_acc += (output == label).float().sum().item()\n",
    "    num_batches += 1\n",
    "test_time = time.time()\n",
    "test_speed = test_samples / (test_time - start_time)\n",
    "test_loss /= test_samples\n",
    "test_acc /= test_samples\n",
    "\n",
    "print(f'test_loss ={test_loss: .4f}, test_acc ={test_acc: .4f}')\n",
    "print(f'test speed ={test_speed: .4f} images/s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
